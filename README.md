# Papers

## Uncertainty Estimation in Deep Learning
1. [To Trust Or Not To Trust A Classifier](https://arxiv.org/abs/1805.11783) `Google`
2. [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474) `DeepMind`
3. [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142) `Cambridge`
4. [Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers](https://openreview.net/forum?id=SJfb5jCqKm) `Technion`
5. [Can You Trust Your Modelâ€™s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://arxiv.org/abs/1906.02530) `Google`
6. [Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning](https://arxiv.org/abs/2002.06470) `Samsung`
7. [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599) `Cornell`
8. [What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?](https://arxiv.org/abs/1703.04977) `Cambridge`

## Uncertainty Estimation in Regression
1. [PIVEN: A Deep Neural Network for Prediction Intervals with Specific Value Prediction](https://arxiv.org/abs/2006.05139) `Ben-Gurion University`
2. [Accurate Uncertainties for Deep Learning Using Calibrated Regression](https://arxiv.org/abs/1807.00263)  `Stanford`
3. [High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach](https://arxiv.org/abs/1802.07167) `Cambridge`
4. [Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel](https://arxiv.org/abs/1906.00588) `Cognizant`
5. [Single-Model Uncertainties for Deep Learning](https://arxiv.org/abs/1811.00908) `Facebook`
6. [Prediction Intervals: Split Normal Mixture from Quality-Driven Deep Ensembles](https://arxiv.org/abs/2007.09670) `Norwegian University`
7. [Adaptive, Distribution-Free Prediction Intervals for Deep Networks](https://arxiv.org/abs/1905.10634) `Vienna University`


## Selective Prediction 
1. [Deep Gamblers: Learning to Abstain with Portfolio Theory](https://arxiv.org/abs/1907.00208) `Carnegie Mellon`
2. [Selective Classification for Deep Neural Networks](https://arxiv.org/abs/1705.08500) `Technion`
3. [SelectiveNet: A Deep Neural Network with an Integrated Reject Option](https://arxiv.org/abs/1901.09192) `Technion`

## AutoML
1. [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) `Google`
2. [MnasNet: Platform-Aware Neural Architecture Search for Mobile](https://arxiv.org/abs/1807.11626) `Google`

## Vision
1. [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://arxiv.org/abs/2001.07685) `Google`
2. [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722) `Facebook`
3. [AutoAugment: Learning Augmentation Policies from Data](https://arxiv.org/abs/1805.09501) `Google`
4. [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719) `Google`
5. [Perturbative Neural Networks](https://arxiv.org/abs/1806.01817) `Carnegie Mellon`

## Miscellaneous
1. [On The Power of Curriculum Learning in Training Deep Networks](https://arxiv.org/abs/1904.03626) `Hebrew University`

