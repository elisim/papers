# Papers

## Uncertainty Estimation in Deep Learning
1. [To Trust Or Not To Trust A Classifier](https://arxiv.org/abs/1805.11783) `Google`
2. [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474) `DeepMind`
3. [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142) `Cambridge`
4. [Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers](https://openreview.net/forum?id=SJfb5jCqKm) `Technion`
5. [Can You Trust Your Modelâ€™s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://arxiv.org/abs/1906.02530) `Google`
6. [Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning](https://arxiv.org/abs/2002.06470) `Samsung`
7. [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599) `Cornell`
8. [What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?](https://arxiv.org/abs/1703.04977) `Cambridge`
9. [A Review of Uncertainty Quantification in Deep
Learning: Techniques, Applications and
Challenges](https://arxiv.org/abs/2011.06225) `IEEE`

## Uncertainty Estimation in Regression
1. [PIVEN: A Deep Neural Network for Prediction Intervals with Specific Value Prediction](https://arxiv.org/abs/2006.05139) `Ben-Gurion University`
2. [Accurate Uncertainties for Deep Learning Using Calibrated Regression](https://arxiv.org/abs/1807.00263)  `Stanford`
3. [High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach](https://arxiv.org/abs/1802.07167) `Cambridge`
4. [Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation](https://arxiv.org/abs/1906.00588) `Cognizant`
5. [Single-Model Uncertainties for Deep Learning](https://arxiv.org/abs/1811.00908) `Facebook`
6. [Prediction Intervals: Split Normal Mixture from Quality-Driven Deep Ensembles](https://arxiv.org/abs/2007.09670) `Norwegian University`
7. [Adaptive, Distribution-Free Prediction Intervals for Deep Networks](https://arxiv.org/abs/1905.10634) `Vienna University`
8. [Conformalized Quantile Regression](https://arxiv.org/abs/1905.03222) `Stanford`

## ML Engineering
1. [Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX)](https://arxiv.org/abs/2010.02013) `Google`
2. [Challenges in Deploying Machine Learning: a Survey of Case Studies](https://arxiv.org/abs/2011.09926) `Cambridge`
3. [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf) `Google`

## Selective Prediction 
1. [Deep Gamblers: Learning to Abstain with Portfolio Theory](https://arxiv.org/abs/1907.00208) `Carnegie Mellon`
2. [Selective Classification for Deep Neural Networks](https://arxiv.org/abs/1705.08500) `Technion`
3. [SelectiveNet: A Deep Neural Network with an Integrated Reject Option](https://arxiv.org/abs/1901.09192) `Technion`

## AutoML
1. [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) `Google`
2. [MnasNet: Platform-Aware Neural Architecture Search for Mobile](https://arxiv.org/abs/1807.11626) `Google`

## Vision
1. [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://arxiv.org/abs/2001.07685) `Google`
2. [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722) `Facebook`
3. [AutoAugment: Learning Augmentation Policies from Data](https://arxiv.org/abs/1805.09501) `Google`
4. [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719) `Google`
5. [Perturbative Neural Networks](https://arxiv.org/abs/1806.01817) `Carnegie Mellon`

## Curriculum Learning
1. [On The Power of Curriculum Learning in Training Deep Networks](https://arxiv.org/abs/1904.03626) `Hebrew University`
2. [Self-Paced Learning for Latent Variable Models](https://papers.nips.cc/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf) `Stanford`

## Time-Series
1. [Time-Series Anomaly Detection Service at Microsoft](https://arxiv.org/abs/1906.03821) `Microsoft`
2. [ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels](https://arxiv.org/abs/1910.13051) `Monash University`
3. [sktime: A Unified Interface for Machine Learning
with Time Series](http://learningsys.org/neurips19/assets/papers/sktime_ml_systems_neurips2019.pdf) `Alan Turing Institute`


